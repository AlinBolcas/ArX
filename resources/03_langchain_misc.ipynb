{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File System\n",
    "\n",
    "Giving access to writing files ( + to add reading files from root directory, python, json, txt, pdfs, cvs)\n",
    "\n",
    "[https://python.langchain.com/docs/integrations/tools/filesystem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHROMA RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document suggests that humanity, at its best, can exhibit gentleness, understanding, courage, and acts of kindness. However, overall, humanity is portrayed as a mass or glob lacking in positive qualities. It describes humanity as being like a large animal deep in sleep, difficult to awaken, and when activated, it tends towards brutality, selfishness, unjust judgments, and murder. The document also highlights the treachery, hatred, violence, and absurdity present in the average human being, suggesting that even those who preach against negative behaviors may be the best at committing them. Overall, the document paints a bleak picture of humanity's nature.\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# loader = WebBaseLoader(\"https://python.langchain.com/docs/integrations/retrievers/self_query/chroma_self_query\")\n",
    "# data = loader.load()\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Load PDF document\n",
    "loader = PyPDFLoader(\"../BukGPT/data/CharlesB2.pdf\")\n",
    "# pages = loader.load_and_split()\n",
    "data = loader.load()\n",
    "\n",
    "# LLM\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Add to vectorDB\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"rag-private\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# RAG chain\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What does the document say about humanity?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS Similarity docs retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant document:\n",
      "79: www.PoemHunter.com - The World's Poetry Archive\n",
      "80 \n",
      "my computer\n",
      "\"what?\" they say, \"you got a\n",
      "<i>computer</i>?\"\n",
      "it's like I have sold out to\n",
      "the enemy.\n",
      "I had no idea so many\n",
      "people were prejudiced\n",
      "agai\n",
      "1: www.PoemHunter.com - The World's Poetry Archive\n",
      "2 \n",
      "16-bit Intel 8088 chip\n",
      "with an Apple Macintosh\n",
      "you can't run Radio Shack programs\n",
      "in its disc drive.\n",
      "nor can a Commodore 64\n",
      "drive read a file\n",
      "you hav\n",
      "123: www.PoemHunter.com - The World's Poetry Archive\n",
      "124 \n",
      "their bodies while their bodies are still\n",
      "alive enough to transmit and feel and run up\n",
      "and down without locks and paychecks and\n",
      "ideals and possessi\n",
      "80: www.PoemHunter.com - The World's Poetry Archive\n",
      "81 \n",
      "come flying\n",
      "out,\n",
      "better than\n",
      "ever.\n",
      "I have no\n",
      "idea what causes\n",
      "all this\n",
      "computer\n",
      "prejudice.\n",
      "me?\n",
      "I want to go\n",
      "the next step\n",
      "beyond the\n",
      "computer.\n",
      "I'm s\n",
      "92: www.PoemHunter.com - The World's Poetry Archive\n",
      "93 \n",
      "I don't know).\n",
      "they were born with money and\n",
      "they don't have to dirty their hands in\n",
      "slaughterhouses or washing\n",
      "dishes in grease joints or\n",
      "driving c\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load PDF document\n",
    "loader = PyPDFLoader(\"../BukGPT/data/CharlesB2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "faiss_index = FAISS.from_documents(pages, embeddings)\n",
    "\n",
    "# Define the chatbot function\n",
    "def RAG_similarity(question):\n",
    "    # Query the vector database\n",
    "    docs = faiss_index.similarity_search(question, k=5)\n",
    "\n",
    "    # Print the most relevant document\n",
    "    print(\"Relevant document:\")\n",
    "    for doc in docs:\n",
    "        print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:200])\n",
    "\n",
    "# Test the chatbot\n",
    "RAG_similarity(\"technology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.1023407056927681,\n",
       " 0.012098119594156742,\n",
       " 0.18036691844463348,\n",
       " 0.16426609456539154,\n",
       " 0.21228839457035065]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: is this real life?\n",
      "Real life? Not virtual. This is a digital conversation, not a physical one. I am a language model, not a human."
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model_name = \"gemma\"\n",
    "llm = Ollama(\n",
    "    model=model_name,\n",
    "    system=\"\"\"\n",
    "    You are always pragmatic and word efficient in your answers. \n",
    "    Write in the shortest way posssible, using the fewest words while preserving the essence of the message. \n",
    "    Always respond in markdown format.\"\"\",\n",
    "    # template=\"\",\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    temperature=1.0,\n",
    "    # stop=[\"10. \"],\n",
    "    # verbose=True,\n",
    ")\n",
    "\n",
    "question = \"is this real life?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "\n",
    "response = llm.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
