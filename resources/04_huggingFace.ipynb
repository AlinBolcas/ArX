{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Image Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers transformers accelerate --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SD Turbo: Text to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipe.to(\"mps\")\n",
    "\n",
    "prompt = \"an abstract figurative scupture depicting humanity, black marble\"\n",
    "\n",
    "for i in range(3):\n",
    "    image = pipe(prompt=prompt, num_inference_steps=3, guidance_scale=0.5).images[0]\n",
    "    image=image.resize((1024,1024))\n",
    "    display(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SD Turbo: Image to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForImage2Image, DiffusionPipeline\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "\n",
    "pipe = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipe.to(\"mps\")\n",
    "\n",
    "# load both base & refiner\n",
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "base.to(\"mps\")\n",
    "\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "refiner.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "init_image = load_image(\"https://rack01powering2.expertagent.co.uk/(S(w5vn2dffgx4lnlrgpuu1tv4r))/agencies/%7Bcb3e4606-1032-41d6-a3ce-b23a61fcc618%7D/%7B4f5bc5d4-5d0f-4679-8304-f0d12e7813ee%7D/main/IMG_99649AmyStreet.jpg\").resize((512, 512))\n",
    "\n",
    "\n",
    "\n",
    "display(init_image)\n",
    "\n",
    "prompt = \"scandinavian elegant interior design, white walls, wooden floors, large windows, plants, minimalistic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    # image = pipe(prompt, image=init_image, num_inference_steps=3, strength=.5, guidance_scale=0.5, denoising_end=.8, output_type=\"latent\",).images[0]\n",
    "    \n",
    "    image = pipe(prompt, image=init_image, num_inference_steps=2, strength=.5, guidance_scale=.5, denoising_end=.8).images[0]\n",
    "    image=image.resize((1024, 1024))\n",
    "\n",
    "    \n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined = refiner(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=6,\n",
    "    denoising_start=.85,\n",
    "    image=image).images[0]\n",
    "display(refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate Through Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists for the parameters to iterate through\n",
    "strength_values = [0.9, 0.7, 0.5]\n",
    "num_inference_steps_values = [1, 2, 3]\n",
    "guidance_scale_values = [0, 0.5, 1]\n",
    "\n",
    "# Iterate through the parameters and generate comparison images\n",
    "for strength_value in strength_values:\n",
    "    for num_steps in num_inference_steps_values:\n",
    "        for scale_value in guidance_scale_values:\n",
    "            # Generate the image using the constant prompt and varying parameters\n",
    "            if (num_steps*strength_value) >= 1:\n",
    "                image = pipe(\n",
    "                    prompt=prompt,\n",
    "                    image=init_image,\n",
    "                    num_inference_steps=num_steps,\n",
    "                    strength=strength_value,\n",
    "                    guidance_scale=scale_value,\n",
    "                    denoising_end=0.8\n",
    "                ).images[0]\n",
    "            \n",
    "                # Resize the image if needed\n",
    "                # image = image.resize((1024, 1024))\n",
    "                \n",
    "                # Display or save the generated image here\n",
    "                print (f\"num_steps: {num_steps}, strength_value: {strength_value}, scale_value: {scale_value}\")\n",
    "                display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X1 SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
    "pipe.to(\"mps\")\n",
    "\n",
    "# if using torch < 2.0\n",
    "# pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "prompt = \"add elements to the garden, a bench, a pond, grass\"\n",
    "\n",
    "images = pipe(prompt=prompt, \n",
    "              image=init_image, \n",
    "            #   negative_prompt=\"dog\", \n",
    "              num_inference_steps=5, \n",
    "              guidance_scale=3, \n",
    "              strength=1).images[0]\n",
    "\n",
    "display(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + Refiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# load both base & refiner\n",
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "base.to(\"mps\")\n",
    "\n",
    "\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "refiner.to(\"mps\")\n",
    "\n",
    "# Define how many steps and what % of steps to be run on each experts (80/20) here\n",
    "n_steps = 20\n",
    "high_noise_frac = 0.8\n",
    "\n",
    "# prompt = \"A majestic lion jumping from a big stone at night\"\n",
    "\n",
    "# run both experts\n",
    "image = base(\n",
    "    prompt=prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=n_steps,\n",
    "    denoising_end=high_noise_frac,\n",
    "    output_type=\"latent\",\n",
    ").images\n",
    "image = refiner(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=n_steps,\n",
    "    denoising_start=high_noise_frac,\n",
    "    image=image,\n",
    ").images[0]\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SD Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/kashif/diffusers.git\n",
    "# @wuerstchen-v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n",
    "\n",
    "device = \"cpu\"\n",
    "num_images_per_prompt = 1\n",
    "\n",
    "prior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", torch_dtype=torch.bfloat16).to(device)\n",
    "decoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\",  torch_dtype=torch.float16).to(device)\n",
    "\n",
    "prompt = \"Anthropomorphic cat dressed as a pilot\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "prior_output = prior(\n",
    "    prompt=prompt,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    negative_prompt=negative_prompt,\n",
    "    guidance_scale=1.0,\n",
    "    num_images_per_prompt=num_images_per_prompt,\n",
    "    num_inference_steps=10\n",
    ")\n",
    "decoder_output = decoder(\n",
    "    image_embeddings=prior_output.image_embeddings.half(),\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    guidance_scale=0.0,\n",
    "    output_type=\"pil\",\n",
    "    num_inference_steps=5\n",
    ").images\n",
    "\n",
    "for image in decoder_output:\n",
    "    display(image)\n",
    "\n",
    "#Now decoder_output is a list with your PIL images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate -q\n",
    "!pip install git+https://github.com/huggingface/diffusers@@shap-ee\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import ShapEPipeline\n",
    "from diffusers.utils import export_to_gif\n",
    "\n",
    "\n",
    "ckpt_id = \"openai/shap-e\"\n",
    "pipe = ShapEPipeline.from_pretrained(repo).to(\"cuda\")\n",
    "\n",
    "\n",
    "guidance_scale = 15.0\n",
    "prompt = \"a shark\"\n",
    "images = pipe(\n",
    "    prompt,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_inference_steps=64,\n",
    "    size=256,\n",
    ").images\n",
    "\n",
    "gif_path = export_to_gif(images, \"shark_3d.gif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Music Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import scipy\n",
    "\n",
    "synthesiser = pipeline(\"text-to-audio\", \"facebook/musicgen-small\")\n",
    "\n",
    "music = synthesiser(\"lo-fi music with a soothing melody\", forward_params={\"do_sample\": True}, max_new_tokens =50)\n",
    "\n",
    "scipy.io.wavfile.write(\"musicgen_out.wav\", rate=music[\"sampling_rate\"], data=music[\"audio\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\").to(\"cpu\")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"soothing ambient insturmental meditation music\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\", \n",
    ")\n",
    "\n",
    "audio_values = model.generate(**inputs, max_new_tokens=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "Audio(audio_values[0].numpy(), rate=sampling_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "scipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=audio_values[0, 0].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, EulerDiscreteScheduler\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "base = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "repo = \"ByteDance/SDXL-Lightning\"\n",
    "ckpt = \"sdxl_lightning_4step_lora.safetensors\" # Use the correct ckpt for your step setting!\n",
    "\n",
    "# Load model.\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(base, torch_dtype=torch.float16, variant=\"fp16\").to(\"cpu\")\n",
    "pipe.load_lora_weights(hf_hub_download(repo, ckpt))\n",
    "pipe.fuse_lora()\n",
    "\n",
    "# Ensure sampler uses \"trailing\" timesteps.\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "# Ensure using the same inference steps as the loaded model and CFG set to 0.\n",
    "pipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "base = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "repo = \"ByteDance/SDXL-Lightning\"\n",
    "ckpt = \"sdxl_lightning_4step_unet.safetensors\" # Use the correct ckpt for your step setting!\n",
    "\n",
    "# Load model.\n",
    "unet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(\"cpu\", torch.float16)\n",
    "unet.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=\"cpu\"))\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "# Ensure sampler uses \"trailing\" timesteps.\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "# Ensure using the same inference steps as the loaded model and CFG set to 0.\n",
    "pipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TTS\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'TTS'"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TTS\n\u001b[1;32m      2\u001b[0m tts \u001b[38;5;241m=\u001b[39m TTS(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtts_models/multilingual/multi-dataset/xtts_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# generate speech by cloning a voice using default settings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'TTS'"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
    "\n",
    "# generate speech by cloning a voice using default settings\n",
    "tts.tts_to_file(text=\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "                file_path=\"output.wav\",\n",
    "                speaker_wav=\"/output/speaker.wav\",\n",
    "                language=\"en\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### SENTIMENTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
