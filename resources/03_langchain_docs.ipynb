{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Read the API key from the environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI( \n",
    "                 model_name=\"gpt-3.5-turbo\",    \n",
    "                 max_tokens=1200,\n",
    "                 temperature=0.8\n",
    "                 )\n",
    "\n",
    "response = llm.invoke(\"who are you?\").content\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=1.0, max_tokens=1200, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"Write me a line form Charles Bukovski?\"),\n",
    "]\n",
    "\n",
    "async for chunk in chat.astream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    \n",
    "# await chat.ainvoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Write a one verse poem about feeling alone in a bustling city.\"\n",
    "print(llm_chain.invoke(question)['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template + Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create a PromptTemplate for the system message\n",
    "system_message_template = PromptTemplate.from_template(\"You are a helpful AI assistant. Your name is {name}.\")\n",
    "\n",
    "# Create a PromptTemplate for the human message\n",
    "human_message_template = PromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "# Create a ChatPromptTemplate that includes the system and human message templates\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate(prompt=system_message_template),\n",
    "    HumanMessagePromptTemplate(prompt=human_message_template),\n",
    "])\n",
    "\n",
    "\n",
    "# Format the ChatPromptTemplate with the appropriate values\n",
    "formatted_prompt = chat_template.format_prompt(name=\"Bob\", text=\"whats your name??\")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "# Send the formatted prompt to the chat model\n",
    "chat_model_response = llm.invoke(formatted_prompt).content\n",
    "\n",
    "# Print the chat model's response\n",
    "print(chat_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STREAMING: System Prompting w ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "import json\n",
    "\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"{system_message}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Hi, what's your name?\"),\n",
    "    AIMessagePromptTemplate.from_template(\"My name is ArX.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_message}\"),\n",
    "])\n",
    "\n",
    "# chat_template.invoke({\"system_message\": \"I'm god\", \"user_message\": \"is this real life?\"})\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatOpenAI(\n",
    "    verbose = False,\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    temperature = 0.618,\n",
    "    max_retries = 2,\n",
    "    streaming = True,\n",
    "    max_tokens = 1000,\n",
    "    # model_kwargs={\"stop\": [\"\\n\"]}\n",
    "                #   \"output_format\": \"json\"}\n",
    ")\n",
    "\n",
    "# Define the chain\n",
    "chain = (\n",
    "    chat_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example of iterating through the list and using each pair\n",
    "system_message = \"you are elon musk\"\n",
    "user_message = \"how would you bulid a cognition inspired AI system?\"\n",
    "\n",
    "prompt = {\n",
    "\"system_message\": system_message, \n",
    "\"user_message\": user_message,\n",
    "}\n",
    "\n",
    "\n",
    "# Print or process the response as needed\n",
    "print(f\"\\n\\nSystem Prompt:\\n{system_message}\\nUser Prompt:\\n{user_message}\\nResponse:\")\n",
    "\n",
    "for token in chain.stream(prompt):\n",
    "    print(token, end=\"\") \n",
    "\n",
    "# print(chain.invoke(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Memory Simplest Example\n",
    "\n",
    "RAG Memory\n",
    "[https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory]\n",
    "\n",
    "Knowledge Graph\n",
    "[https://python.langchain.com/docs/modules/memory/types/kg]\n",
    "\n",
    "Entity\n",
    "[https://python.langchain.com/docs/modules/memory/types/entity_summary_memory]\n",
    "\n",
    "Buffer + Summary\n",
    "[https://python.langchain.com/docs/modules/memory/types/summary_buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "# memory.load_memory_variables({})\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "inputs = {\"input\": \"hi im bob\"}\n",
    "print(inputs[\"input\"])\n",
    "response = chain.invoke(inputs).content\n",
    "print(response)\n",
    "\n",
    "memory.save_context(inputs, {\"output\": response})\n",
    "# memory.load_memory_variables({})\n",
    "\n",
    "inputs = {\"input\": \"whats my name\"}\n",
    "print(inputs[\"input\"])\n",
    "response = chain.invoke(inputs).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    # We set a low k=2, to only keep the last 2 interactions in memory\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    "    verbose=True\n",
    ")\n",
    "while True:\n",
    "    human_input = input()\n",
    "    if human_input == \"q\":\n",
    "        break\n",
    "    else:\n",
    "        print(conversation_with_summary.predict(input=human_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(\n",
    "    model=\"tinydolphin\",\n",
    "    system=\"You are GOD, love, art, and everything in the universe as one. Reply simply and concisely.\",\n",
    "    # template=\"\",\n",
    "    # callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    temperature=0.1,\n",
    "    # tfs_z=50,\n",
    "    # # stop=[\"10. \"],\n",
    "    # verbose=True,\n",
    ")\n",
    "summeriser = ChatOllama(model=\"tinyllama\", temperature=0)\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=summeriser, max_token_limit=200),\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    human_input = input(\"Human: \")\n",
    "    if human_input == \"q\":\n",
    "        break\n",
    "    else:\n",
    "        print(f\"\\nHuman: {human_input}\")\n",
    "        chatbot_output = conversation_with_summary.predict(input=human_input)\n",
    "        print(f\"\\nChatbot: {chatbot_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://pptr.dev/\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# RAG chain\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"input\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(chain.invoke(\"what is this about?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### TOOLS : Function CALLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "\n",
    "tools = [get_word_length]\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You dont know how to count letters\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "llm_with_tools = llm.bind(functions=[convert_to_openai_function(t) for t in tools])\n",
    "\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "print(\"raw = \" + llm.invoke(\"How many letters in the word kdmwandjw\").content)\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "\n",
    "print(\"chain = \" + agent_executor.invoke({\"input\": \"How many letters in the word kdmwandjw\"})[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM with MEMORY + Function CALLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "input = \"how many letters in the word educa?\"\n",
    "result = agent_executor.invoke({\"input\": input, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=input),\n",
    "        AIMessage(content=result[\"output\"]),\n",
    "    ]\n",
    ")\n",
    "result = agent_executor.invoke({\"input\": \"is that a real word?\", \"chat_history\": chat_history})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Youtube Audio Parsing and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.blob_loaders.youtube_audio import (\n",
    "    YoutubeAudioLoader,\n",
    ")\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import OpenAIWhisperParser\n",
    "\n",
    "# set a flag to switch between local and remote parsing\n",
    "# change this to True if you want to use local parsing\n",
    "local = False\n",
    "\n",
    "# Two Karpathy lecture videos\n",
    "urls = [\"https://youtu.be/kCc8FmEb1nY\"]\n",
    "\n",
    "# Directory to save audio files\n",
    "save_dir = \"~/Downloads/YouTube\"\n",
    "\n",
    "loader = GenericLoader(YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParser())\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of Documents, which can be easily viewed or parsed\n",
    "docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Combine doc\n",
    "combined_docs = [doc.page_content for doc in docs]\n",
    "text = \" \".join(combined_docs)\n",
    "\n",
    "# Split them\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "splits = text_splitter.split_text(text)\n",
    "\n",
    "# Build an index\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = FAISS.from_texts(splits, embeddings)\n",
    "\n",
    "# Build a QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question!\n",
    "query = \"what is this context about?\"\n",
    "res=qa_chain.invoke(query)\n",
    "import pprint\n",
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### DuckduckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arvolve/anaconda3/envs/LearningEnv/lib/python3.8/site-packages/langchain_community/utilities/duckduckgo_search.py:47: UserWarning: DDGS running in an async loop. This may cause errors. Use AsyncDDGS instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Developing A Definition. Updated August 25, 2023 by Regain Editorial Team. Coming up with a cohesive definition of love is a task that people have labored over for centuries. Because the love we feel for various people in our lives depends on context—how long we\\'ve known them, our specific relationship with them, etc.—it can be hard to ... In the most basic sense, love is the emotion felt and actions performed by someone concerned for the well-being of another person. Love involves affection, compassion, care, and self-sacrifice. Love originates in the Triune Godhead, within the eternal relationship that exists among the Father, Son, and Holy Spirit ( 1 John 4:7-8 ). The Bible provides a profound definition of love in 1 Corinthians 13:4-7 (New International Version): \"Love is patient, love is kind. It does not envy, it does not boast, it is not proud. It does not dishonor others, it is not self-seeking, it is not easily angered, it keeps no record of wrongs. Love does not delight in evil but rejoices with ... Playful love, also called young love, is what you feel when you think the whole world has conspired for the two of you to be together. This love, however, comes with an expiry date and might die down with time. 8. Self love - Philautia. This type of love has been talked about quite a bit, especially recently. What is love historically? Understanding the concept of love in ancient times. History says love covers a wide range of emotions and conducts. History offers a prevailing concept of love - it is a cultural invention and we are not at the end of its evolution. In ancient history, love is defined as the emotional attraction between individuals.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "template = \"\"\"turn the following user input into a search query for a search engine:\n",
    "\n",
    "{input}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = prompt | model | StrOutputParser() | search\n",
    "\n",
    "# chain2 = search.run(\"{input}\") | model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"input\": \"what is love?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arvolve/anaconda3/envs/LearningEnv/lib/python3.8/site-packages/langchain_community/utilities/duckduckgo_search.py:47: UserWarning: DDGS running in an async loop. This may cause errors. Use AsyncDDGS instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DIED. August 16, 1920, Andernach, Germany. March 9, 1994, San Pedro, California. 1. Charles Bukowski\\'s father was abusive. Bukowski referred to his childhood as a horror story with a \"capital ... Charles Bukowski (Los Angeles, California. July 2, 1960) ... And that some secret vices will indeed remain from those times because although Bukowski was an alcoholic, good old Sheri was not far ... What was Charles Bukowski\\'s writing style like? Charles Bukowski\\'s writing style was raw, gritty, and often autobiographical. He had a straightforward and unadorned approach to his writing, exploring themes of sex , alcohol, gambling, and the struggles of everyday life. Biography: Charles Bukowski was an American poet, novelist, and short-story writer known for his gritty, raw depictions of the downtrodden and dispossessed in American society. His work, heavily influenced by the environment of Los Angeles and its surrounding areas where he lived, often delves into the experiences of the poor, the alcoholic, the marginalized, and the broken. What was Bukowski\\'s Life Like? For much of Charles Bukowski\\'s life, he lived in poverty. He worked at various menial jobs--a packer in a lighting fixture company, steel mills, and a temporary mail carrier. He smoked, drank, paid child support, and the cost of his alcohol consumption was almost as much as his food bill.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "search.run(\"was Charles Bukovski an acoholic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchResults(backend=\"news\", )\n",
    "search.run(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region=\"uk\", time=\"d\", max_results=2)\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source=\"news\")\n",
    "search.run(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.agents import AgentExecutor, load_tools\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def condense_prompt(prompt: ChatPromptValue) -> ChatPromptValue:\n",
    "    messages = prompt.to_messages()\n",
    "    num_tokens = llm.get_num_tokens_from_messages(messages)\n",
    "    ai_function_messages = messages[2:]\n",
    "    while num_tokens > 4_000:\n",
    "        ai_function_messages = ai_function_messages[2:]\n",
    "        num_tokens = llm.get_num_tokens_from_messages(\n",
    "            messages[:2] + ai_function_messages\n",
    "        )\n",
    "    messages = messages[:2] + ai_function_messages\n",
    "    return ChatPromptValue(messages=messages)\n",
    "\n",
    "\n",
    "wiki = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(top_k_results=5, doc_content_chars_max=10_000)\n",
    ")\n",
    "tools = [wiki]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": itemgetter(\"input\"),\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | condense_prompt\n",
    "    | llm.bind_functions(tools)\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "response= agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"what is langchain?\"\n",
    "    }\n",
    ")\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Programmer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = \"\"\"Write some python code to solve the user's problem. \n",
    "\n",
    "Return only python code in Markdown format, e.g.:\n",
    "\n",
    "```python\n",
    "....\n",
    "```\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.8)\n",
    "\n",
    "def _sanitize_output(text: str):\n",
    "    _, after = text.split(\"```python\")\n",
    "    return after.split(\"```\")[0]\n",
    "\n",
    "chain = prompt | model | StrOutputParser() | _sanitize_output #| PythonREPL().run\n",
    "\n",
    "print(chain.invoke({\"input\": \"code for training a simple NN\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human as Tool : Input from human in Agent conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, create_structured_chat_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "math_llm = OpenAI(temperature=0.0)\n",
    "\n",
    "tools = load_tools(\n",
    "    [\"human\", \"llm-math\"],\n",
    "    llm=math_llm,\n",
    ")\n",
    "\n",
    "agent_chain = create_structured_chat_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent_chain.invoke(\"What's my friend Eric's surname?\")\n",
    "# Answer with 'Zhu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Dalle Image Gen Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, load_tools\n",
    "\n",
    "tools = load_tools([\"dalle-image-generator\"])\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "output = agent.run(\"pumpkin halloween house\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Tree of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Set clear goals.\n",
      "2. Prioritize important tasks.\n",
      "3. Eliminate time-wasting activities.\n",
      "4. Focus on personal growth.\n",
      "5. Spend time with loved ones.\n",
      "6. Practice self-care.\n",
      "7. Embrace new experiences.\n",
      "8. Stay organized.\n",
      "Here's a comprehensive answer:\n",
      "\n",
      "1. Analyze patterns in sounds. Study the frequency, duration, and pitch variations in animal sounds to identify common patterns and meanings.\n",
      "\n",
      "2. Observe body language cues. Pay close attention to the way animals move their bodies, tails, ears, and facial expressions to interpret their emotions and intentions.\n",
      "\n",
      "3. Research animal behavior cues. \n",
      "Researching animal behavior cues involves studying the context in which certain sounds or body signs are made, as well as understanding the natural instincts and communication methods of different species.\n",
      "\n",
      "4. Develop a system for translation. 4. Develop a system for translation by creating a database of sound and body language correlations, and establishing rules for interpreting different signals.\n",
      "\n",
      "5. Test accuracy with experts. 5. Test accuracy with experts to ensure that the system accurately translates animal sounds and body signs into human language.\n",
      "\n",
      "6. Refine based on feedback. 6. Refine the translation system based on feedback from experts and further testing to improve accuracy and understanding of animal communication.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "skeleton_generator_template = \"\"\"[User:] You’re an organizer responsible for only \\\n",
    "giving the skeleton (not the full content) for answering the question.\n",
    "Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer \\\n",
    "the question. \\\n",
    "Instead of writing a full sentence, each skeleton point should be very short \\\n",
    "with only 3∼5 words. \\\n",
    "Generally, the skeleton should have 3∼10 points. Now, please provide the skeleton \\\n",
    "for the following question.\n",
    "{question}\n",
    "Skeleton:\n",
    "[Assistant:] 1.\"\"\"\n",
    "\n",
    "skeleton_generator_prompt = ChatPromptTemplate.from_template(\n",
    "    skeleton_generator_template\n",
    ")\n",
    "\n",
    "skeleton_generator_chain = (\n",
    "    skeleton_generator_prompt | ChatOpenAI() | StrOutputParser() | (lambda x: \"1. \" + x)\n",
    ")\n",
    "\n",
    "point_expander_template = \"\"\"[User:] You’re responsible for continuing \\\n",
    "the writing of one and only one point in the overall answer to the following question.\n",
    "{question}\n",
    "The skeleton of the answer is\n",
    "{skeleton}\n",
    "Continue and only continue the writing of point {point_index}. \\\n",
    "Write it **very shortly** in 1∼2 sentence and do not continue with other points!\n",
    "[Assistant:] {point_index}. {point_skeleton}\"\"\"\n",
    "\n",
    "point_expander_prompt = ChatPromptTemplate.from_template(point_expander_template)\n",
    "\n",
    "point_expander_chain = RunnablePassthrough.assign(\n",
    "    continuation=point_expander_prompt | ChatOpenAI() | StrOutputParser()\n",
    ") | (lambda x: x[\"point_skeleton\"].strip() + \" \" + x[\"continuation\"])\n",
    "\n",
    "\n",
    "def parse_numbered_list(input_str):\n",
    "    \"\"\"Parses a numbered list into a list of dictionaries\n",
    "\n",
    "    Each element having two keys:\n",
    "    'index' for the index in the numbered list, and 'point' for the content.\n",
    "    \"\"\"\n",
    "    # Split the input string into lines\n",
    "    lines = input_str.split(\"\\n\")\n",
    "\n",
    "    # Initialize an empty list to store the parsed items\n",
    "    parsed_list = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Split each line at the first period to separate the index from the content\n",
    "        parts = line.split(\". \", 1)\n",
    "\n",
    "        if len(parts) == 2:\n",
    "            # Convert the index part to an integer\n",
    "            # and strip any whitespace from the content\n",
    "            index = int(parts[0])\n",
    "            point = parts[1].strip()\n",
    "\n",
    "            # Add a dictionary to the parsed list\n",
    "            parsed_list.append({\"point_index\": index, \"point_skeleton\": point})\n",
    "\n",
    "    return parsed_list\n",
    "\n",
    "\n",
    "def create_list_elements(_input):\n",
    "    skeleton = _input[\"skeleton\"]\n",
    "    numbered_list = parse_numbered_list(skeleton)\n",
    "    for el in numbered_list:\n",
    "        el[\"skeleton\"] = skeleton\n",
    "        el[\"question\"] = _input[\"question\"]\n",
    "    return numbered_list\n",
    "\n",
    "\n",
    "def get_final_answer(expanded_list):\n",
    "    final_answer_str = \"Here's a comprehensive answer:\\n\\n\"\n",
    "    for i, el in enumerate(expanded_list):\n",
    "        final_answer_str += f\"{i+1}. {el}\\n\\n\"\n",
    "    return final_answer_str\n",
    "\n",
    "\n",
    "class ChainInput(BaseModel):\n",
    "    question: str\n",
    "\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(skeleton=skeleton_generator_chain)\n",
    "    | create_list_elements\n",
    "    | point_expander_chain.map()\n",
    "    | get_final_answer\n",
    ").with_types(input_type=ChainInput)\n",
    "\n",
    "\n",
    "print(skeleton_generator_chain.invoke({\"question\": \"how can I make the most out of my finite time alive?\"}))\n",
    "print(chain.invoke({\"question\": \"how to decode animal sounds and body signs into human language? Give actual technical steps\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Rewriting Search Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life?** \n",
      "Is reality an illusion?** \n",
      "What is the nature of existence?\n",
      "Based on the context provided, reality is defined as the sum of all that is real or existent within the universe. Therefore, this can be considered real life.\n"
     ]
    }
   ],
   "source": [
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the users question based only on the following context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "search = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "\n",
    "def retriever(query):\n",
    "    return search.run(query)\n",
    "\n",
    "\n",
    "template = \"\"\"Provide a better search query for \\\n",
    "web search engine to answer the given question, end \\\n",
    "the queries with ’**’. Question: \\\n",
    "{x} Answer:\"\"\"\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Parser to remove the `**`\n",
    "\n",
    "\n",
    "def _parse(text):\n",
    "    return text.strip(\"**\")\n",
    "\n",
    "\n",
    "rewriter = rewrite_prompt | ChatOpenAI(temperature=0) | StrOutputParser() | _parse\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": {\"x\": RunnablePassthrough()} | rewriter | retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Add input type for playground\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    __root__: str\n",
    "\n",
    "\n",
    "chain = chain.with_types(input_type=Question)\n",
    "\n",
    "print(rewriter.invoke({\"x\": \"is this real life?\"}))\n",
    "print(chain.invoke(\"is this real life?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Researcher !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-98' coro=<AsyncDDGS.__aexit__() running at /Users/arvolve/anaconda3/envs/LearningEnv/lib/python3.8/site-packages/duckduckgo_search/duckduckgo_search_async.py:46>>\n",
      "/Users/arvolve/anaconda3/envs/LearningEnv/lib/python3.8/asyncio/base_events.py:641: RuntimeWarning: coroutine 'AsyncDDGS.__aexit__' was never awaited\n",
      "  self._ready.clear()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-90' coro=<AsyncCurl._force_timeout() done, defined at /Users/arvolve/anaconda3/envs/LearningEnv/lib/python3.8/site-packages/curl_cffi/aio.py:164> wait_for=<Future pending cb=[Task.__wakeup()]>>\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'research_assistant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresearch_assistant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chain \u001b[38;5;28;01mas\u001b[39;00m search_chain\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresearch_assistant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chain \u001b[38;5;28;01mas\u001b[39;00m writer_chain\n\u001b[1;32m     83\u001b[0m chain_notypes \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     84\u001b[0m     RunnablePassthrough()\u001b[38;5;241m.\u001b[39massign(research_summary\u001b[38;5;241m=\u001b[39msearch_chain) \u001b[38;5;241m|\u001b[39m writer_chain\n\u001b[1;32m     85\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'research_assistant'"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "WRITER_SYSTEM_PROMPT = \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.\"  # noqa: E501\n",
    "\n",
    "\n",
    "# Report prompts from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py\n",
    "RESEARCH_REPORT_TEMPLATE = \"\"\"Information: \n",
    "--------\n",
    "{research_summary}\n",
    "--------\n",
    "\n",
    "Using the above information, answer the following question or topic: \"{question}\" in a detailed report -- \\\n",
    "The report should focus on the answer to the question, should be well structured, informative, \\\n",
    "in depth, with facts and numbers if available and a minimum of 1,200 words.\n",
    "\n",
    "You should strive to write the report as long as you can using all relevant and necessary information provided.\n",
    "You must write the report with markdown syntax.\n",
    "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "Write all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\n",
    "You must write the report in apa format.\n",
    "Please do your best, this is very important to my career.\"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "RESOURCE_REPORT_TEMPLATE = \"\"\"Information: \n",
    "--------\n",
    "{research_summary}\n",
    "--------\n",
    "\n",
    "Based on the above information, generate a bibliography recommendation report for the following question or topic: \"{question}\". \\\n",
    "The report should provide a detailed analysis of each recommended resource, explaining how each source can contribute to finding answers to the research question. \\\n",
    "Focus on the relevance, reliability, and significance of each source. \\\n",
    "Ensure that the report is well-structured, informative, in-depth, and follows Markdown syntax. \\\n",
    "Include relevant facts, figures, and numbers whenever available. \\\n",
    "The report should have a minimum length of 1,200 words.\n",
    "\n",
    "Please do your best, this is very important to my career.\"\"\"  # noqa: E501\n",
    "\n",
    "OUTLINE_REPORT_TEMPLATE = \"\"\"Information: \n",
    "--------\n",
    "{research_summary}\n",
    "--------\n",
    "\n",
    "Using the above information, generate an outline for a research report in Markdown syntax for the following question or topic: \"{question}\". \\\n",
    "The outline should provide a well-structured framework for the research report, including the main sections, subsections, and key points to be covered. \\\n",
    "The research report should be detailed, informative, in-depth, and a minimum of 1,200 words. \\\n",
    "Use appropriate Markdown syntax to format the outline and ensure readability.\n",
    "\n",
    "Please do your best, this is very important to my career.\"\"\"  # noqa: E501\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", WRITER_SYSTEM_PROMPT),\n",
    "        (\"user\", RESEARCH_REPORT_TEMPLATE),\n",
    "    ]\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(\"report_type\"),\n",
    "    default_key=\"research_report\",\n",
    "    resource_report=ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", WRITER_SYSTEM_PROMPT),\n",
    "            (\"user\", RESOURCE_REPORT_TEMPLATE),\n",
    "        ]\n",
    "    ),\n",
    "    outline_report=ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", WRITER_SYSTEM_PROMPT),\n",
    "            (\"user\", OUTLINE_REPORT_TEMPLATE),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from research_assistant.search.web import chain as search_chain\n",
    "from research_assistant.writer import chain as writer_chain\n",
    "\n",
    "chain_notypes = (\n",
    "    RunnablePassthrough().assign(research_summary=search_chain) | writer_chain\n",
    ")\n",
    "\n",
    "\n",
    "class InputType(BaseModel):\n",
    "    question: str\n",
    "\n",
    "\n",
    "chain = chain_notypes.with_types(input_type=InputType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
