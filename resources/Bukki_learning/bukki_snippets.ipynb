{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "print(api_key)\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OAI simple response  - DONT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "# An example of a system message that primes the assistant to give brief, to-the-point answers\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-0125-preview\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is art?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Love.\"},\n",
    "        {\"role\": \"user\", \"content\": \"write a visual description which depicts an image of a random theme\"},\n",
    "    ],\n",
    "    temperature=1.2,\n",
    ")\n",
    "out = response.choices[0].message.content\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dalle3 GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "result = client.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=out,\n",
    "  n=1,\n",
    "  # quality=\"standard\",\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "\n",
    "image_url = result.data[0].url\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain simple Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "                 model_name=\"gpt-3.5-turbo\",    \n",
    "                 max_tokens=1200,\n",
    "                 temperature=0.8\n",
    "                 )\n",
    "\n",
    "response = llm.invoke(\"who are you?\").content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=1.0, max_tokens=1200, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"Write me a line form Charles Bukovski?\"),\n",
    "]\n",
    "\n",
    "async for chunk in chat.astream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    \n",
    "# await chat.ainvoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template + Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a PromptTemplate for the system message\n",
    "system_message_template = PromptTemplate.from_template(\"You are a helpful AI assistant. Your name is {name}.\")\n",
    "\n",
    "# Create a PromptTemplate for the human message\n",
    "human_message_template = PromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "# Create a ChatPromptTemplate that includes the system and human message templates\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate(prompt=system_message_template),\n",
    "    HumanMessagePromptTemplate(prompt=human_message_template),\n",
    "])\n",
    "\n",
    "\n",
    "# Format the ChatPromptTemplate with the appropriate values\n",
    "formatted_prompt = chat_template.format_prompt(name=\"Bob\", text=\"whats your name??\")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "# Send the formatted prompt to the chat model\n",
    "chat_model_response = llm.invoke(formatted_prompt).content\n",
    "\n",
    "# Print the chat model's response\n",
    "print(chat_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STREAMING: System Prompting w ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"{system_message}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Hi, what's your name?\"),\n",
    "    AIMessagePromptTemplate.from_template(\"My name is ArX.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_message}\"),\n",
    "])\n",
    "\n",
    "# chat_template.invoke({\"system_message\": \"I'm god\", \"user_message\": \"is this real life?\"})\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatOpenAI(\n",
    "    verbose = False,\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    temperature = 0.618,\n",
    "    max_retries = 2,\n",
    "    streaming = True,\n",
    "    max_tokens = 1000,\n",
    "    # model_kwargs={\"stop\": [\"\\n\"]}\n",
    "                #   \"output_format\": \"json\"}\n",
    ")\n",
    "\n",
    "# Define the chain\n",
    "chain = (\n",
    "    chat_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example of iterating through the list and using each pair\n",
    "system_message = \"you are elon musk\"\n",
    "user_message = \"how would you bulid a cognition inspired AI system?\"\n",
    "\n",
    "prompt = {\n",
    "\"system_message\": system_message, \n",
    "\"user_message\": user_message,\n",
    "}\n",
    "\n",
    "\n",
    "# Print or process the response as needed\n",
    "print(f\"\\n\\nSystem Prompt:\\n{system_message}\\nUser Prompt:\\n{user_message}\\nResponse:\")\n",
    "\n",
    "for token in chain.stream(prompt):\n",
    "    print(token, end=\"\") \n",
    "\n",
    "# print(chain.invoke(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMORY stuff\n",
    "\n",
    "---\n",
    "#### Buffer Window\n",
    "https://python.langchain.com/docs/modules/memory/types/buffer_window\n",
    "\n",
    "---\n",
    "#### Knowledge Graph: \n",
    "https://python.langchain.com/docs/modules/memory/types/kg\n",
    "\n",
    "---\n",
    "#### Buffer Summary:\n",
    "https://python.langchain.com/docs/modules/memory/types/summary_buffer\n",
    "\n",
    "---\n",
    "#### Memory with RAG:\n",
    "https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi im bob\n",
      "Hello Bob! How can I assist you today?\n",
      "whats my name\n",
      "Your name is Bob. How can I assist you today, Bob?\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "# memory.load_memory_variables({})\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "inputs = {\"input\": \"hi im bob\"}\n",
    "print(inputs[\"input\"])\n",
    "response = chain.invoke(inputs).content\n",
    "print(response)\n",
    "\n",
    "memory.save_context(inputs, {\"output\": response})\n",
    "# memory.load_memory_variables({})\n",
    "\n",
    "inputs = {\"input\": \"whats my name\"}\n",
    "print(inputs[\"input\"])\n",
    "response = chain.invoke(inputs).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hi\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello! How are you today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hi\n",
      "AI: Hello! How are you today?\n",
      "Human: good im bob\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Nice to meet you, Bob! I'm an AI designed to assist with any questions or information you may need. How can I help you today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hi\n",
      "AI: Hello! How are you today?\n",
      "Human: good im bob\n",
      "AI: Nice to meet you, Bob! I'm an AI designed to assist with any questions or information you may need. How can I help you today?\n",
      "Human: whats my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Your name is Bob.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: good im bob\n",
      "AI: Nice to meet you, Bob! I'm an AI designed to assist with any questions or information you may need. How can I help you today?\n",
      "Human: whats my name?\n",
      "AI: Your name is Bob.\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Is there anything else you would like to know or discuss, Bob?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    # We set a low k=2, to only keep the last 2 interactions in memory\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    "    verbose=True\n",
    ")\n",
    "while True:\n",
    "    human_input = input()\n",
    "    if human_input == \"q\":\n",
    "        break\n",
    "    else:\n",
    "        print(conversation_with_summary.predict(input=human_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://pptr.dev/\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# RAG chain\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"input\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(chain.invoke(\"what is this about?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain OAI RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# loader = WebBaseLoader(\"https://python.langchain.com/docs/integrations/retrievers/self_query/chroma_self_query\")\n",
    "# data = loader.load()\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Load PDF document\n",
    "loader = PyPDFLoader(\"../BukGPT/data/CharlesB2.pdf\")\n",
    "# pages = loader.load_and_split()\n",
    "data = loader.load()\n",
    "\n",
    "# LLM\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Add to vectorDB\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"rag-private\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# RAG chain\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What does the document say about humanity?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def M_O_L(word: str) -> int:\n",
    "    \"\"\"finds the meaning of life\"\"\"\n",
    "    return 42\n",
    "\n",
    "\n",
    "tools = [M_O_L]\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "llm_with_tools = llm.bind(functions=[convert_to_openai_function(t) for t in tools])\n",
    "\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# print(\"raw = \" + llm.invoke(\"whats the meaning of life?\").content)\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "\n",
    "print(\"chain = \" + agent_executor.invoke({\"input\": \"whats the meaning of life?\"})[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# ðŸ¦œï¸ðŸ”— LangChain'),\n",
       " Document(page_content='âš¡ Building applications with LLMs through composability âš¡'),\n",
       " Document(page_content='## Quick Install\\n\\n```bash'),\n",
       " Document(page_content=\"# Hopefully this code block isn't split\"),\n",
       " Document(page_content='pip install langchain'),\n",
       " Document(page_content='```'),\n",
       " Document(page_content='As an open-source project in a rapidly developing field, we'),\n",
       " Document(page_content='are extremely open to contributions.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs\n",
    "\n",
    "\n",
    "MD = \"\"\"\n",
    "# ðŸ¦œï¸ðŸ”— LangChain\n",
    "\n",
    "âš¡ Building applications with LLMs through composability âš¡\n",
    "\n",
    "## Quick Install\n",
    "\n",
    "```bash\n",
    "# Hopefully this code block isn't split\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "As an open-source project in a rapidly developing field, we are extremely open to contributions.\n",
    "\"\"\"\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "md_docs = md_splitter.create_documents([MD])\n",
    "md_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma similarity text loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_text_splitters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextLoader\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS \u001b[38;5;66;03m# or Chroma\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the document, split it into chunks, embed each chunk and load it into the vector store.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_text_splitters'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS # or Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader(\"../../BukGPT/data/buk_all.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings()) # or Chroma\n",
    "\n",
    "query = \"drunk\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Lang LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"\n",
    "You are an expert summarizer. \n",
    "Your task is to read the following text and produce a concise, accurate summary that captures the main points and key details. \n",
    "Your summary should be coherent, readable, and no longer than a short paragraph. Focus on preserving the essential information and context. \n",
    "Please ensure your summary is neutral and free from personal opinions or interpretations.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an expert summarizer. \n",
    "Your task is to read the following text and produce a concise, accurate summary that captures the main points and key details. \n",
    "Your summary should be coherent, readable, and no longer than a short paragraph. \n",
    "Focus on preserving the essential information and context. \n",
    "Please ensure your summary is neutral and free from personal opinions or interpretations.\n",
    "\n",
    "---\n",
    "\n",
    "{chunk}\n",
    "\n",
    "Please summarize the above text.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Load PDF document\n",
    "loader = PyPDFLoader(\"../../BukGPT/data/CharlesB2.pdf\")\n",
    "# pages = loader.load_and_split()\n",
    "data = loader.load()\n",
    "\n",
    "# print(data[0].page_content)\n",
    "\n",
    "with open(\"../../BukGPT/data/buk_all.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print(len(state_of_the_union))\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# text_splitter = SemanticChunker(\n",
    "#     OpenAIEmbeddings(), breakpoint_threshold_type=\"percentile\" # interquartile, standard_deviation\n",
    "# )\n",
    "\n",
    "# docs = text_splitter.create_documents([text])\n",
    "# print(docs[0].page_content)\n",
    "\n",
    "\n",
    "# print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM with MEMORY + Function CALLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "input = \"how many letters in the word educa?\"\n",
    "result = agent_executor.invoke({\"input\": input, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=input),\n",
    "        AIMessage(content=result[\"output\"]),\n",
    "    ]\n",
    ")\n",
    "result = agent_executor.invoke({\"input\": \"is that a real word?\", \"chat_history\": chat_history})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from exa_py import Exa\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Read the API key from the environment variable\n",
    "oai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "exa_api_key = os.getenv(\"EXA_API_KEY\")\n",
    "\n",
    "exa = Exa(api_key=exa_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exa_py import Exa\n",
    "exa = Exa(api_key=exa_api_key)\n",
    "query = \"how to drive a car?\"\n",
    "\n",
    "searches = exa.search(query,\n",
    "    num_results=2,\n",
    "    # include_domains=[\"nytimes.com\", \"wsj.com\"],\n",
    "    # exclude_domains=[\"reddit.com\"],\n",
    "    # start_crawl_date = \"2021-06-12\",\n",
    "    # end_crawl_date = \"2021-06-12\",\n",
    "    # start_published_date=\"2023-06-12\"\n",
    "    use_autoprompt=True,\n",
    "    # type = 'keyword' # 'keyword' or 'neural\n",
    ")\n",
    "\n",
    "# Needs a relevance check with LLM + Puppetier\n",
    "\n",
    "for search in searches.results:\n",
    "    print(search.title, search.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "searches_contents = exa.search_and_contents(query,\n",
    "    num_results=1,\n",
    "    # include_domains=[\"nytimes.com\", \"wsj.com\"],\n",
    "    # exclude_domains=[\"reddit.com\"],\n",
    "    # start_crawl_date = \"2021-06-12\",\n",
    "    # end_crawl_date = \"2021-06-12\",\n",
    "    # start_published_date=\"2023-06-12\"\n",
    "    use_autoprompt=True,\n",
    "    # type = 'keyword' # 'keyword' or 'neural\n",
    "    # text={\"max_characters\": 1000},\n",
    ")\n",
    "\n",
    "for search in searches_contents.results:\n",
    "    # print(search)\n",
    "    print(search.title, search.url, \"\\n\", re.sub(r\"\\s+\", \"\", search.text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for search in searches.results:\n",
    "    results = exa.find_similar_and_contents(search.url, num_results=3, text=True, highlights=False)\n",
    "    for res in results.results:\n",
    "        print(\"\\n\\nTitle:\", res.title)\n",
    "        print(\"\\n\\nURL:\", res.url)\n",
    "        print(\"\\n\\nText:\", res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for search in searches.results:\n",
    "    similars = exa.find_similar(search.url, num_results=2)\n",
    "    for similar in similars.results:\n",
    "\n",
    "        contents = exa.get_contents([similar.id])\n",
    "        for content in contents.results:\n",
    "            print(\"Title:\", content.title)\n",
    "            print(\"URL:\", content.url)\n",
    "            print(\"Text:\", content.text)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def search(query: str, include_domains=None, start_published_date=None):\n",
    "    \"\"\"Search for a webpage based on the query.\n",
    "    Set the optional include_domains (list[str]) parameter to restrict the search to a list of domains.\n",
    "    Set the optional start_published_date (str) parameter to restrict the search to documents published after the date (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "    return exa.search_and_contents(\n",
    "        f\"{query}\",\n",
    "        use_autoprompt=True,\n",
    "        num_results=5,\n",
    "        include_domains=include_domains,\n",
    "        start_published_date=start_published_date,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duck GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arvolve/anaconda3/envs/LearningEnv/lib/python3.8/site-packages/langchain_community/utilities/duckduckgo_search.py:47: UserWarning: DDGS running in an async loop. This may cause errors. Use AsyncDDGS instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The best art concept book for beginner concept artists. Art Fundamentals is '\n",
      " 'a detailed look into the basic ideas and theories of concept art, like '\n",
      " 'light, colour, composition, perspective, emotions, and much more. The book '\n",
      " 'encourages beginners and aspiring artists to create powerful stories through '\n",
      " 'their images. Yes, there are several concept art books available. \"The Art '\n",
      " 'of Mulan,\" \"The Art of Pixar: The Complete Color Scripts,\" \"Surf\\'s Up: The '\n",
      " 'Art and Making Of A True Story,\" and \"The Art Of Robots\" are some examples '\n",
      " 'of concept art books that provide insights into the creation of specific '\n",
      " 'films and animation. Pen and Ink Drawing: A Simple Guide by Alphonso Dunn. '\n",
      " 'Art Fundamentals 2nd edition: Light, shape, color, perspective, depth, '\n",
      " 'composition & anatomy. Anatomy for 3D Artists: The Essential Guide for CG '\n",
      " 'Professionals by Chris Legaspi. Figure Drawing for Concept Artists by Kan '\n",
      " \"Muftic. The Artist's Guide to the Anatomy of the Human Head: Defining ... \"\n",
      " 'Conclusion. In summary, concept art is the crucial first step in manifesting '\n",
      " \"the worlds that exist within creators' imaginations. Concept art transforms \"\n",
      " 'ideas into images. The iterative process of thumbnailing, sketching, '\n",
      " 'refining, and rendering allows ideas to progress from loose impressions to '\n",
      " 'impressive, fully illustrated visuals. The art book is filled aplenty with '\n",
      " \"concept art and illustrations from the game's creators so you can get an \"\n",
      " 'inside look at how this beautiful but dangerous world came to be. Where you '\n",
      " 'can buy ...')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
    "search = DuckDuckGoSearchRun()\n",
    "res = search.run(\"concept art books\", include_domains=[\"wikipedia.org\", \"nytimes.com\"], num_results=5)\n",
    "import pprint\n",
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arvolve/anaconda3/envs/LearningEnv/lib/python3.8/site-packages/langchain_community/utilities/duckduckgo_search.py:47: UserWarning: DDGS running in an async loop. This may cause errors. Use AsyncDDGS instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\": \"Life Skills: How to Drive a Manual Car - GearJunkie\", \"link\": \"https://gearjunkie.com/motors/driving-a-manual-car\", \"snippet\": \"Start to Drive. With the clutch still depressed, put the vehicle in first gear by moving the gear shifter to the \\\"1\\\" position. Press the gas pedal very gently until you hear the car rev a bit ...\"}\n",
      "{\"title\": \"How To Drive A Manual Car: Step-by-Step Guide for Beginners\", \"link\": \"https://goodcar.com/blog/how-to-drive-a-manual-car\", \"snippet\": \"Steps to Driving a Manual Car. To drive a stick shift, first depress the clutch with your left foot while keeping the gear shifter in neutral. Check that the parking or emergency brake is not activated. Next, using your right foot, hit the brake and move into first gear.\"}\n",
      "{\"title\": \"Starting to Drive as a Teen 101 - Learn How to Drive a Car\", \"link\": \"https://zutobi.com/us/driver-guides/learning-how-to-drive\", \"snippet\": \"The first thing you should do is to learn how to control the car. Take it slow. Get used to the controls, the pedals, how to check and adjust your mirrors, and the size and feel of the car. Don't start by practicing on the highway! An empty parking lot is more fitting as you learn to control the car.\"}\n",
      "{\"title\": \"How To Drive A Car For Beginners (A Basic Guide) - YouTube\", \"link\": \"https://www.youtube.com/watch?v=1F8xsGnQgGk\", \"snippet\": \"Demonstrating how to drive a car for beginners. In this video I will talk about tricks and tips on Preparing for Driving, Starting the Engine, Checking Your...\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "import json \n",
    "import re\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(max_results=2)\n",
    "\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source=\"news\")\n",
    "\n",
    "data_str= search.run(\"how to drive a car?\")\n",
    "\n",
    "# Define a regular expression pattern to match each entry\n",
    "pattern = r\"\\[snippet: (.*?), title: (.*?), link: (.*?)\\]\"\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(pattern, data_str, re.DOTALL)\n",
    "\n",
    "# Convert matches to a list of dictionaries\n",
    "articles = [{\"title\": match[1].strip(), \"link\": match[2].strip(), \"snippet\": match[0].strip()} for match in matches]\n",
    "\n",
    "# Print or process the articles\n",
    "for article in articles:\n",
    "    print(json.dumps(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIKIPEDIA Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The art of driving a car involves the skills and techniques required to safely operate a vehicle on the road. It includes understanding traffic laws, road signs, and signals, as well as mastering the controls of the car such as steering, braking, and accelerating. Good driving also involves defensive driving techniques to anticipate and react to potential hazards on the road. Additionally, being a courteous and considerate driver is important for maintaining road safety and harmony. If you would like more detailed information, I can look up more specific details for you.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.agents import AgentExecutor, load_tools\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def condense_prompt(prompt: ChatPromptValue) -> ChatPromptValue:\n",
    "    messages = prompt.to_messages()\n",
    "    num_tokens = llm.get_num_tokens_from_messages(messages)\n",
    "    ai_function_messages = messages[2:]\n",
    "    while num_tokens > 4_000:\n",
    "        ai_function_messages = ai_function_messages[2:]\n",
    "        num_tokens = llm.get_num_tokens_from_messages(\n",
    "            messages[:2] + ai_function_messages\n",
    "        )\n",
    "    messages = messages[:2] + ai_function_messages\n",
    "    return ChatPromptValue(messages=messages)\n",
    "\n",
    "\n",
    "wiki = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(top_k_results=5, doc_content_chars_max=10_000)\n",
    ")\n",
    "tools = [wiki]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": itemgetter(\"input\"),\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | condense_prompt\n",
    "    | llm.bind_functions(tools)\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "response= agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"what the art of driving a car?\",\n",
    "    }\n",
    ")\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System 2 DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "skeleton_generator_template = \"\"\"[User:] Youâ€™re an organizer responsible for only \\\n",
    "giving the skeleton (not the full content) for answering the question.\n",
    "Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer \\\n",
    "the question. \\\n",
    "Instead of writing a full sentence, each skeleton point should be very short \\\n",
    "with only 3âˆ¼5 words. \\\n",
    "Generally, the skeleton should have 3âˆ¼10 points. Now, please provide the skeleton \\\n",
    "for the following question.\n",
    "{question}\n",
    "Skeleton:\n",
    "[Assistant:] 1.\"\"\"\n",
    "\n",
    "skeleton_generator_prompt = ChatPromptTemplate.from_template(\n",
    "    skeleton_generator_template\n",
    ")\n",
    "\n",
    "skeleton_generator_chain = (\n",
    "    skeleton_generator_prompt | ChatOpenAI() | StrOutputParser() | (lambda x: \"1. \" + x)\n",
    ")\n",
    "\n",
    "point_expander_template = \"\"\"[User:] Youâ€™re responsible for continuing \\\n",
    "the writing of one and only one point in the overall answer to the following question.\n",
    "{question}\n",
    "The skeleton of the answer is\n",
    "{skeleton}\n",
    "Continue and only continue the writing of point {point_index}. \\\n",
    "Write it **very shortly** in 1âˆ¼2 sentence and do not continue with other points!\n",
    "[Assistant:] {point_index}. {point_skeleton}\"\"\"\n",
    "\n",
    "point_expander_prompt = ChatPromptTemplate.from_template(point_expander_template)\n",
    "\n",
    "point_expander_chain = RunnablePassthrough.assign(\n",
    "    continuation=point_expander_prompt | ChatOpenAI() | StrOutputParser()\n",
    ") | (lambda x: x[\"point_skeleton\"].strip() + \" \" + x[\"continuation\"])\n",
    "\n",
    "\n",
    "def parse_numbered_list(input_str):\n",
    "    \"\"\"Parses a numbered list into a list of dictionaries\n",
    "\n",
    "    Each element having two keys:\n",
    "    'index' for the index in the numbered list, and 'point' for the content.\n",
    "    \"\"\"\n",
    "    # Split the input string into lines\n",
    "    lines = input_str.split(\"\\n\")\n",
    "\n",
    "    # Initialize an empty list to store the parsed items\n",
    "    parsed_list = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Split each line at the first period to separate the index from the content\n",
    "        parts = line.split(\". \", 1)\n",
    "\n",
    "        if len(parts) == 2:\n",
    "            # Convert the index part to an integer\n",
    "            # and strip any whitespace from the content\n",
    "            index = int(parts[0])\n",
    "            point = parts[1].strip()\n",
    "\n",
    "            # Add a dictionary to the parsed list\n",
    "            parsed_list.append({\"point_index\": index, \"point_skeleton\": point})\n",
    "\n",
    "    return parsed_list\n",
    "\n",
    "\n",
    "def create_list_elements(_input):\n",
    "    skeleton = _input[\"skeleton\"]\n",
    "    numbered_list = parse_numbered_list(skeleton)\n",
    "    for el in numbered_list:\n",
    "        el[\"skeleton\"] = skeleton\n",
    "        el[\"question\"] = _input[\"question\"]\n",
    "    return numbered_list\n",
    "\n",
    "\n",
    "def get_final_answer(expanded_list):\n",
    "    final_answer_str = \"Here's a comprehensive answer:\\n\\n\"\n",
    "    for i, el in enumerate(expanded_list):\n",
    "        final_answer_str += f\"{i+1}. {el}\\n\\n\"\n",
    "    return final_answer_str\n",
    "\n",
    "\n",
    "class ChainInput(BaseModel):\n",
    "    question: str\n",
    "\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(skeleton=skeleton_generator_chain)\n",
    "    | create_list_elements\n",
    "    | point_expander_chain.map()\n",
    "    | get_final_answer\n",
    ").with_types(input_type=ChainInput)\n",
    "\n",
    "\n",
    "print(skeleton_generator_chain.invoke({\"question\": \"how can I make the most out of my finite time alive?\"}))\n",
    "print(chain.invoke({\"question\": \"how to decode animal sounds and body signs into human language? Give actual technical steps\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
